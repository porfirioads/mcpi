{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4a4fa0-c52f-4b94-98df-9c428e21c7d8",
   "metadata": {},
   "source": [
    "**OPTIMIZATION IN SENTIMENT ANALYSIS**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34786e06-4a73-4222-9d2c-bb2e9030103f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb02d2-a370-4689-8cde-c07a3cb7c952",
   "metadata": {},
   "source": [
    "# Comandos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab2341-5ae5-418d-99e3-434146fc7e68",
   "metadata": {},
   "source": [
    "## Instalación de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aa664-7fd4-49fb-aefa-3fefbb0a3f85",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda install -c anaconda nltk\n",
    "conda install -c anaconda pandas\n",
    "conda install -c conda-forge optuna\n",
    "conda install -c anaconda scikit-learn\n",
    "conda install -c conda-forge xgboost\n",
    "conda install -c conda-forge keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71e853-c86b-4fb8-b766-6d0a043e4bdb",
   "metadata": {},
   "source": [
    "## Solución de errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c0ecc-57d6-4015-b32d-c325c2af713b",
   "metadata": {},
   "source": [
    "**Anaconda deja de funcionar:**\n",
    "\n",
    "```bash\n",
    "conda config --set channel_priority true\n",
    "conda config --set channel_priority false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f50a69-2ffe-475f-a48e-95d23149f692",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a2b95-2bb3-43b8-98cf-7c86f562ce97",
   "metadata": {},
   "source": [
    "# Data preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9bfb07d-46d2-4519-8c83-ce203b49b115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/porfirio/Workspace/mcpi/optimizacion-ejercicios'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27e0ed-f778-4c90-8122-3467a0869522",
   "metadata": {},
   "source": [
    "Not every part of the tweet is important for the text processing we do. Some aspects of the tweet like numbers, symbols, stopwords are not so useful for sentiment analysis.\n",
    "\n",
    "So we just remove them in the preprocessing step. I used nltk python library and regular expressions to remove stopwords, emails, URLs, numbers, white spaces, punctuations, special characters and Unicode data.\n",
    "\n",
    "The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6667ba-4251-4244-8547-ba904fe094fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/porfirio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#read data\n",
    "df = pd.read_csv('proyecto_final/train.csv')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "url = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\n",
    "(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([\n",
    "  ^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "def clean_data(temp):\n",
    "    temp = temp.map(lambda x: str(x).lower())  #lower case\n",
    "    temp = temp.map(lambda x: re.sub(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\", \"\", x))  #email\n",
    "    temp = temp.map(lambda x: re.sub(url, \"\", x))  #url\n",
    "    temp = temp.map(lambda x: re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \"\", x))  #numbers\n",
    "    temp = temp.map(lambda x: re.sub(r'^\\s*|\\s\\s*', ' ', x).strip())  #white space\n",
    "    temp = temp.map(lambda x: ''.join([c for c in x if c not in string.punctuation]))  #punctuations\n",
    "    temp = temp.map(lambda x: re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', x))  #special char\n",
    "    temp = temp.map(\n",
    "        lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore'))  #unicode\n",
    "    temp = temp.map(lambda x: tokenizer.tokenize(x))\n",
    "    temp = temp.map(lambda x: [i for i in x if i not in stopwords.words('english')])\n",
    "    temp = temp.map(lambda x: ' '.join(x))\n",
    "    return temp\n",
    "\n",
    "\n",
    "df.text = clean_data(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f205d5-74bf-41f8-a0e7-d8ef1db96508",
   "metadata": {},
   "source": [
    "As I mentioned before, we are going to use two different methods for sentiment analysis namely, XGBoost Classifier and LSTM neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a764313-1bd7-4d3e-8eaf-17f7e52ca6a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff02b1-f3cb-4906-928f-2193cf1742cb",
   "metadata": {},
   "source": [
    "# XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d8a92-e1c7-45db-ad00-f518f72a2a27",
   "metadata": {},
   "source": [
    "After “cleaning” the text data, the next step is Vectorization. Here, we just convert the text into a numerical format so that the machine learning model can ‘understand’ it.\n",
    "\n",
    "You can observe that data structures such as Text, Images, Graphs etc need to be converted into numerical representations before building an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4e985-cb3c-4200-b654-95d2fc48f160",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7799fc-dfcc-48be-adce-86ea12f46e89",
   "metadata": {},
   "source": [
    "To vectorize the text, we can simply use a Count Vectorizer method from Sci-Kit Learn. Basically, we transform the text into a sparse matrix of unique words where we use numbers to indicate the presence of a word in our text example.\n",
    "\n",
    "We will divide the data into train, validation and test sets in the split ratio of — 80:10:10. The split is stratified so that we have an equal proportion of labels/ sentiments in all data splits.\n",
    "\n",
    "You can use the following code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c460b324-763e-4ccf-bdb9-9f155aea457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vectorization\n",
    "cv = CountVectorizer(lowercase=False)\n",
    "text_vector = cv.fit_transform(df.text.values)\n",
    "\n",
    "x = text_vector\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# train validation test split\n",
    "x_train, xtest, y_train, ytest = train_test_split(x, y, stratify=y,\n",
    "                                                  test_size=0.20, random_state=42)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(xtest, ytest, stratify=ytest,\n",
    "                                                test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c2105-2ba0-47bf-93c4-805c4267c020",
   "metadata": {},
   "source": [
    "## Optuna integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e88c9d-1d87-49b6-910a-a073f41b95a8",
   "metadata": {},
   "source": [
    "In the following code, you will notice an objective function that is being optimized by Optuna. Firstly, we define the hyperparameters that we are interested in tuning and add them to the trial object. Here, I chose to tune learning_rate, max_depth and n_estimators . Depending on the type of hyperparameter, we can use methods such as suggest_float, suggest_int, suggest_categorical .\n",
    "\n",
    "Inside this objective function, we create an instance of the model and fit it on the training set. After training, we predict the sentiment on the validation set and calculate the accuracy metric. The Optuna’s objective function will try to maximize this accuracy score by performing trials with different values of hyperparameters. Different sampling techniques can be employed during this optimization.\n",
    "\n",
    "We can rewrite the objective function to work with the loss value of the model. In this case, we will try minimize the objective function.\n",
    "\n",
    "An early-stopping method is implemented in the form of pruning. The trial will be skipped/ pruned if it seems unpromising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d928b79-bd9e-4fda-aa82-ddea85dcf61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:23,929]\u001b[0m A new study created in memory with name: no-name-4d54152c-9a0d-4a20-a4ec-590b68dcbd94\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:23] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:30,661]\u001b[0m Trial 0 finished with value: 0.5687772925764192 and parameters: {'learning_rate': 4.271962559326317e-05, 'max_depth': 6, 'n_estimators': 300}. Best is trial 0 with value: 0.5687772925764192.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:39,708]\u001b[0m Trial 1 finished with value: 0.5847889374090247 and parameters: {'learning_rate': 0.0020237952677643095, 'max_depth': 8, 'n_estimators': 300}. Best is trial 1 with value: 0.5847889374090247.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:46,672]\u001b[0m Trial 2 finished with value: 0.5822416302765647 and parameters: {'learning_rate': 2.25569185938823e-05, 'max_depth': 8, 'n_estimators': 200}. Best is trial 1 with value: 0.5847889374090247.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:48,482]\u001b[0m Trial 3 finished with value: 0.616448326055313 and parameters: {'learning_rate': 0.05671432738414724, 'max_depth': 4, 'n_estimators': 100}. Best is trial 3 with value: 0.616448326055313.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:48] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:53,760]\u001b[0m Trial 4 finished with value: 0.5687772925764192 and parameters: {'learning_rate': 2.171464586770149e-05, 'max_depth': 6, 'n_estimators': 300}. Best is trial 3 with value: 0.616448326055313.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:53] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:55,635]\u001b[0m Trial 5 finished with value: 0.5251091703056768 and parameters: {'learning_rate': 0.002089127267623584, 'max_depth': 2, 'n_estimators': 200}. Best is trial 3 with value: 0.616448326055313.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:26:57,576]\u001b[0m Trial 6 finished with value: 0.6364628820960698 and parameters: {'learning_rate': 0.061385108846382534, 'max_depth': 6, 'n_estimators': 100}. Best is trial 6 with value: 0.6364628820960698.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:26:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:05,709]\u001b[0m Trial 7 finished with value: 0.643740902474527 and parameters: {'learning_rate': 0.02715941914843331, 'max_depth': 6, 'n_estimators': 300}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:13,380]\u001b[0m Trial 8 finished with value: 0.6029839883551674 and parameters: {'learning_rate': 0.0049612250432105685, 'max_depth': 8, 'n_estimators': 300}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:16,243]\u001b[0m Trial 9 finished with value: 0.5687772925764192 and parameters: {'learning_rate': 0.0007002876973624442, 'max_depth': 6, 'n_estimators': 100}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:24,602]\u001b[0m Trial 10 finished with value: 0.6131732168850073 and parameters: {'learning_rate': 0.00831602300885738, 'max_depth': 10, 'n_estimators': 200}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:26,524]\u001b[0m Trial 11 finished with value: 0.6324599708879185 and parameters: {'learning_rate': 0.0786831855818283, 'max_depth': 4, 'n_estimators': 100}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:26] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:28,384]\u001b[0m Trial 12 finished with value: 0.5909752547307132 and parameters: {'learning_rate': 0.023078436031149036, 'max_depth': 4, 'n_estimators': 100}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:30,177]\u001b[0m Trial 13 finished with value: 0.4959970887918486 and parameters: {'learning_rate': 0.00034747849006004026, 'max_depth': 2, 'n_estimators': 200}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:30] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:36,525]\u001b[0m Trial 14 finished with value: 0.6353711790393013 and parameters: {'learning_rate': 0.02037026812340371, 'max_depth': 10, 'n_estimators': 200}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:36] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:38,429]\u001b[0m Trial 15 finished with value: 0.5906113537117904 and parameters: {'learning_rate': 0.02312789003540615, 'max_depth': 4, 'n_estimators': 100}. Best is trial 7 with value: 0.643740902474527.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:38] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:46,493]\u001b[0m Trial 16 finished with value: 0.6844978165938864 and parameters: {'learning_rate': 0.08454277038040124, 'max_depth': 8, 'n_estimators': 300}. Best is trial 16 with value: 0.6844978165938864.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:27:54,322]\u001b[0m Trial 17 finished with value: 0.5826055312954876 and parameters: {'learning_rate': 0.0001950536630794215, 'max_depth': 8, 'n_estimators': 300}. Best is trial 16 with value: 0.6844978165938864.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:28:07,158]\u001b[0m Trial 18 finished with value: 0.6295487627365357 and parameters: {'learning_rate': 0.008668902168183992, 'max_depth': 10, 'n_estimators': 300}. Best is trial 16 with value: 0.6844978165938864.\u001b[0m\n",
      "/home/porfirio/anaconda3/envs/venv/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:28:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:28:14,406]\u001b[0m Trial 19 finished with value: 0.6874090247452693 and parameters: {'learning_rate': 0.09154420383633016, 'max_depth': 8, 'n_estimators': 300}. Best is trial 19 with value: 0.6874090247452693.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from optuna.trial import TrialState\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# optuna's objective function\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 10, step=2, log=False)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=100, log=False)\n",
    "\n",
    "    model = XGBClassifier(objective='multi:softprob',\n",
    "                          learning_rate=learning_rate,\n",
    "                          n_estimators=n_estimators,\n",
    "                          max_depth=max_depth,\n",
    "                          seed=42)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    trial.set_user_attr(key=\"best_model\", value=model)  # save model\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# callback function to save the best model as user attribute\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])\n",
    "\n",
    "\n",
    "# study to maximize the accuracy metric\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20, timeout=None, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b9517-6800-43bf-889e-8f0f99d3c45f",
   "metadata": {},
   "source": [
    "You might have noticed the set_user_attr method. This is used to save any variable which we might find important. Here we are interested in saving the best model that is associated with the highest validation accuracy. We save the best XGboost model in this user attribute.\n",
    "\n",
    "During the Optuna optimization process, this is what you see:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "The number of trials can be higher if you want Optuna to cover a wider range of hyperparameter values.\n",
    "\n",
    "After the trials have finished, we can retrieve a hyperparameter importance plot which is shown below:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "We observe that learning_rate is a more important hyperparameter than the rest of them. With this, we understand which hyperparameters we need to focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2414f63-e719-472f-867b-61a22c926ced",
   "metadata": {},
   "source": [
    "## Predicting on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71ae7a-9f8c-4c49-8be9-59a823133e04",
   "metadata": {},
   "source": [
    "So we have finished with our model training and hyperparameter tuning. We performed 20 trials to find the optimal hyperparameters. Now we can retrieve our best model and make a prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47fcfb11-991f-44c1-937a-48d3d6d36784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7042560931247727\n"
     ]
    }
   ],
   "source": [
    "# retrieve the best model from optuna study\n",
    "best_model = study.user_attrs['best_model']\n",
    "y_pred = best_model.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3aeb17-bf83-4a51-b684-9ec7078380c0",
   "metadata": {},
   "source": [
    "Not a shabby score! Let’s see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ab344-3504-4e62-96c3-93b09cd7e4ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ecc1e-a708-4169-ae46-e8bf8849ba0e",
   "metadata": {},
   "source": [
    "# LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227823a-06b9-492b-9559-570cad3cb929",
   "metadata": {},
   "source": [
    "Long short-term memory neural network architecture is popular in the domain of Natural Language Processing as it has the capability to retain the sequence information in its “memory”.\n",
    "\n",
    "Just like XGBoost, we should vectorize the text data in order to train the LSTM model. We perform tokenization and then pad the vectorized sequences into the same length.\n",
    "\n",
    "The data is split in a similar fashion to that of the XGBoost model so that we can have a comparison between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137c3a5-dabf-4ca6-adfb-a20af9fbf2e5",
   "metadata": {},
   "source": [
    "## Tokenization and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf1eba0-09e4-463f-89a1-b765815633b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "maxlen = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "x = df.text.values\n",
    "y = df.sentiment.astype(\"category\").cat.codes.values\n",
    "\n",
    "# train validation and test split\n",
    "x_train, xtest, y_train, ytest = train_test_split(x, y, stratify=y,\n",
    "                                                  test_size=0.20,\n",
    "                                                  random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(xtest, ytest,\n",
    "                                                stratify=ytest,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=42)\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#tokenizing and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df.text.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(x_train)\n",
    "X_val = tokenizer.texts_to_sequences(x_val)\n",
    "X_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='pre', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='pre', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='pre', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b711046-dde2-4c08-9b9e-2b15d715da15",
   "metadata": {},
   "source": [
    "Now, we define the LSTM model as follows:\n",
    "    \n",
    "I selected optimizer, epochs and batch_size as the tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422a2591-4c8e-4b89-bc5c-d55cbf99fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "\n",
    "def lstm(optimizer, epochs, batchsize):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        input_length=maxlen))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(LSTM(64, activation=\"tanh\"))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=batchsize)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cea50-b090-48c0-836c-ee518613d496",
   "metadata": {},
   "source": [
    "This neural network model is now ready to train!!\n",
    "\n",
    "Let’s integrate the Optuna to perform the hyperparameter tuning while we train the LSTM model.\n",
    "\n",
    "The code for this Optuna integration looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4af013e-add5-4ca9-85c5-1bdfc67ac3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:28:16,730]\u001b[0m A new study created in memory with name: no-name-8da66392-4d92-4830-a1c3-3f45822cda68\u001b[0m\n",
      "2021-12-01 21:28:16.747785: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-01 21:28:16.981259: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-01 21:28:17.000192: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593260000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0825 - accuracy: 0.4047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:37:05,371]\u001b[0m Trial 0 finished with value: 0.40465793013572693 and parameters: {'optimizer': 'Adadelta', 'epochs': 10, 'batchsize': 8}. Best is trial 0 with value: 0.40465793013572693.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 0.7152 - accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:41:18,755]\u001b[0m Trial 1 finished with value: 0.7099708914756775 and parameters: {'optimizer': 'RMSprop', 'epochs': 10, 'batchsize': 40}. Best is trial 1 with value: 0.7099708914756775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 1.0292 - accuracy: 0.4913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:45:50,045]\u001b[0m Trial 2 finished with value: 0.4912663698196411 and parameters: {'optimizer': 'SGD', 'epochs': 10, 'batchsize': 24}. Best is trial 1 with value: 0.7099708914756775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 1.0607 - accuracy: 0.6820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:52:24,449]\u001b[0m Trial 3 finished with value: 0.6819505095481873 and parameters: {'optimizer': 'adam', 'epochs': 10, 'batchsize': 24}. Best is trial 1 with value: 0.7099708914756775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.7357 - accuracy: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 21:56:47,640]\u001b[0m Trial 4 finished with value: 0.7092430591583252 and parameters: {'optimizer': 'RMSprop', 'epochs': 10, 'batchsize': 40}. Best is trial 1 with value: 0.7099708914756775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.7292 - accuracy: 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:01:05,339]\u001b[0m Trial 5 finished with value: 0.7150654792785645 and parameters: {'optimizer': 'RMSprop', 'epochs': 10, 'batchsize': 40}. Best is trial 5 with value: 0.7150654792785645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 1.0938 - accuracy: 0.4032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:03:16,021]\u001b[0m Trial 6 finished with value: 0.4032023251056671 and parameters: {'optimizer': 'Adadelta', 'epochs': 5, 'batchsize': 24}. Best is trial 5 with value: 0.7150654792785645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 0.7469 - accuracy: 0.7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:14:00,808]\u001b[0m Trial 7 finished with value: 0.7001455426216125 and parameters: {'optimizer': 'RMSprop', 'epochs': 10, 'batchsize': 8}. Best is trial 5 with value: 0.7150654792785645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 1.0826 - accuracy: 0.4047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:22:52,296]\u001b[0m Trial 8 finished with value: 0.40465793013572693 and parameters: {'optimizer': 'Adadelta', 'epochs': 10, 'batchsize': 8}. Best is trial 5 with value: 0.7150654792785645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6898 - accuracy: 0.7158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:28:03,836]\u001b[0m Trial 9 finished with value: 0.7157933115959167 and parameters: {'optimizer': 'RMSprop', 'epochs': 5, 'batchsize': 8}. Best is trial 9 with value: 0.7157933115959167.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.7924 - accuracy: 0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:35:25,299]\u001b[0m Trial 10 finished with value: 0.7063318490982056 and parameters: {'optimizer': 'adam', 'epochs': 5, 'batchsize': 8}. Best is trial 9 with value: 0.7157933115959167.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.7839 - accuracy: 0.7074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:41:35,164]\u001b[0m Trial 11 finished with value: 0.7074235677719116 and parameters: {'optimizer': 'RMSprop', 'epochs': 15, 'batchsize': 40}. Best is trial 9 with value: 0.7157933115959167.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 17ms/step - loss: 0.7699 - accuracy: 0.6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:47:47,370]\u001b[0m Trial 12 finished with value: 0.6997816562652588 and parameters: {'optimizer': 'RMSprop', 'epochs': 15, 'batchsize': 40}. Best is trial 9 with value: 0.7157933115959167.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 1.0701 - accuracy: 0.4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:49:53,036]\u001b[0m Trial 13 finished with value: 0.418122261762619 and parameters: {'optimizer': 'SGD', 'epochs': 5, 'batchsize': 24}. Best is trial 9 with value: 0.7157933115959167.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 0.6900 - accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:52:19,155]\u001b[0m Trial 14 finished with value: 0.7208878993988037 and parameters: {'optimizer': 'RMSprop', 'epochs': 5, 'batchsize': 24}. Best is trial 14 with value: 0.7208878993988037.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 0.7047 - accuracy: 0.7165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 22:57:48,013]\u001b[0m Trial 15 finished with value: 0.7165210843086243 and parameters: {'optimizer': 'RMSprop', 'epochs': 5, 'batchsize': 8}. Best is trial 14 with value: 0.7208878993988037.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 0.6861 - accuracy: 0.7213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 23:00:19,694]\u001b[0m Trial 16 finished with value: 0.7212518453598022 and parameters: {'optimizer': 'RMSprop', 'epochs': 5, 'batchsize': 24}. Best is trial 16 with value: 0.7212518453598022.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 15ms/step - loss: 0.8005 - accuracy: 0.6987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 23:03:34,899]\u001b[0m Trial 17 finished with value: 0.6986899375915527 and parameters: {'optimizer': 'adam', 'epochs': 5, 'batchsize': 24}. Best is trial 16 with value: 0.7212518453598022.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 16ms/step - loss: 1.0669 - accuracy: 0.4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 23:05:47,799]\u001b[0m Trial 18 finished with value: 0.42503640055656433 and parameters: {'optimizer': 'SGD', 'epochs': 5, 'batchsize': 24}. Best is trial 16 with value: 0.7212518453598022.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6914 - accuracy: 0.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-01 23:08:17,794]\u001b[0m Trial 19 finished with value: 0.7219796180725098 and parameters: {'optimizer': 'RMSprop', 'epochs': 5, 'batchsize': 24}. Best is trial 19 with value: 0.7219796180725098.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"SGD\", \"RMSprop\", \"Adadelta\"])\n",
    "    epochs = trial.suggest_int(\"epochs\", 5, 15, step=5, log=False)\n",
    "    batchsize = trial.suggest_int(\"batchsize\", 8, 40, step=16, log=False)\n",
    "\n",
    "    history, model = lstm(optimizer_name, epochs, batchsize)\n",
    "\n",
    "    val_acc = model.evaluate(X_val, y_val)[1]\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    trial.set_user_attr(key=\"best_model_weights\", value=weights)\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_model_weights\",\n",
    "                            value=trial.user_attrs[\"best_model_weights\"])\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20, timeout=None, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfae5d6-6c46-4d45-99e2-c1a6af0d3829",
   "metadata": {},
   "source": [
    "The structure for this Optuna integration is the same. We just change the model and hyperparameters inside the objective function.\n",
    "\n",
    "Similarly, we obtain the hyperparameter importance plot for LSTM:\n",
    "\n",
    "IMAGE\n",
    "\n",
    "We see that optimizer is an important hyperparameter and batch size was not contributing much to the improvement in the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29703a1c-7214-49ae-97c7-6f7d0b957ce9",
   "metadata": {},
   "source": [
    "## Issues that I faced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635311bd-d995-43e7-bf42-747db431b9d1",
   "metadata": {},
   "source": [
    "For XGBoost we could save the model directly but Optuna gives some errors when you are trying to save the Keras model in a similar fashion. From my search, I found that this is because the Keras model is non-pickleable?!\n",
    "\n",
    "A workaround for this is to just save the weights for the best model and then use these weights to reconstruct the model.\n",
    "\n",
    "The following code will explain more about this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0532bc38-b0d6-49cf-bb9f-59b1bc3bbac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'RMSprop', 'epochs': 5, 'batchsize': 24}\n",
      "86/86 [==============================] - 2s 15ms/step - loss: 0.6983 - accuracy: 0.7077\n",
      "0.7115314602851868\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)\n",
    "\n",
    "embedding_dim = 100\n",
    "optimizer = study.best_params['optimizer']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(64, activation=\"tanh\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "best_model_weights = study.user_attrs['best_model_weights']\n",
    "# setting the saved weights to new model\n",
    "model.set_weights(best_model_weights)\n",
    "\n",
    "# evaluating on the test set\n",
    "test_acc = model.evaluate(X_test, y_test)[1]\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30766659-a90a-4796-8d77-cfce5da4b2c8",
   "metadata": {},
   "source": [
    "You will just create a new instance of the model and set the weights retrieved from Optuna, instead of training it again.\n",
    "\n",
    "The test accuracy score obtained with LSTM:\n",
    "\n",
    "XXXX\n",
    "\n",
    "This score is better than XGBoost. Often, neural network methods perform better than standard machine learning methods. We can improve this accuracy score even further by using Transformer architectures such as BERT, RoBERTa or XLNet.\n",
    "\n",
    "Finally, I enjoyed using Optuna for hyperparameter tuning. I could easily retrieve the best model from all the different trials and also understand which hyperparameter is important to tune during the training process (using the hyperparameter importance plot)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
